{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4  2.   3. ]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    x = x.astype(np.float64)\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    x = x.astype(np.float64)\n",
    "    x = np.vstack((np.zeros(x.shape[0]),x))\n",
    "    x = np.max(x,axis=0)\n",
    "    return x\n",
    "def leakyrelu(x,k=0.4):\n",
    "    x = x.astype(np.float64)\n",
    "    x[x < 0.0] = x[x < 0.0]*k\n",
    "    # x[x > 0.0] = x[x > 0.0]\n",
    "    return x\n",
    "def randomrelu(x,low = 0.0,high = 1.0):\n",
    "    x = x.astype(np.float64)\n",
    "    x[x < 0.0] = x[x < 0.0]*np.random.uniform(low,high,None)\n",
    "    #None can be replaced by any size of matrix\n",
    "    return x\n",
    "\n",
    "a = np.array([-1,2,3])\n",
    "print(leakyrelu(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def mseLoss(y_true,y_pred):\n",
    "    return ((y_true-y_pred)**2.0).mean()\n",
    "y_true = np.array([1,0,0,1])\n",
    "y_pred = np.array([0,0,0,0])\n",
    "print(mseLoss(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,weights=0.0,bias=0.0):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward(self,inputs):\n",
    "        total = np.dot(self.weights,inputs) + self.bias\n",
    "        #Hardmard method and sum them together finall add bias\n",
    "        return sigmoid(total)\n",
    "        #the return method is a summary result and have a active function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivsigmoid(x):\n",
    "    # x = x.astype(np.float64)\n",
    "    # x = 1/(1+np.exp(-x))\n",
    "    # x = x*(1-x)\n",
    "    return x*(1-x)\n",
    "# for the input already is sigmoid(x)\n",
    "class NeuralNetwork:\n",
    "    # 2 inputs\n",
    "    # with a hidden layer\n",
    "    def __init__(self):\n",
    "        self.h1 = Neuron([0,0],0)\n",
    "        self.h2 = Neuron([0,0],0)\n",
    "        self.o1 = Neuron([0,0],0)\n",
    "    \n",
    "    def feedforward(self,x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1,out_h2]))\n",
    "        # print(self.o1.weights[0],self.o1.weights[1]) show the real weights in o1\n",
    "        return out_o1\n",
    "    \n",
    "    def train(self,data,all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for x,y_true in zip(data,all_y_trues):\n",
    "                out_h1 = self.h1.feedforward(x)\n",
    "                out_h2 = self.h2.feedforward(x)\n",
    "                out_o1 = self.o1.feedforward(np.array([out_h1,out_h2]))\n",
    "                \n",
    "                y_pred = out_o1\n",
    "                d_L_d_ypred = -2*(y_true-y_pred)\n",
    "                # because of the MSE LOSS which means L(Loss) equals (y_true-y_pred)**2\n",
    "                \n",
    "                #Neuron o1\n",
    "                d_ypred_d_o1w1 = out_h1*derivsigmoid(out_o1)\n",
    "                d_ypred_d_o1w2 = out_h2*derivsigmoid(out_o1)\n",
    "                d_ypred_d_b3 = derivsigmoid(out_o1)\n",
    "                \n",
    "                d_ypred_d_h1 = self.o1.weights[0] * derivsigmoid(out_o1)\n",
    "                d_ypred_d_h2 = self.o1.weights[1] * derivsigmoid(out_o1)\n",
    "                \n",
    "                #Neuron h1\n",
    "                d_h1_d_h1w1 = x[0] * derivsigmoid(out_h1)\n",
    "                d_h1_d_h1w2 = x[1] * derivsigmoid(out_h1)\n",
    "                d_h1_d_b1 = derivsigmoid(out_h1)\n",
    "                \n",
    "                #Neuron h2\n",
    "                d_h2_d_h2w1 = x[0] * derivsigmoid(out_h2)\n",
    "                d_h2_d_h2w2 = x[1] * derivsigmoid(out_h2)\n",
    "                d_h2_d_b2 = derivsigmoid(out_h2)\n",
    "                \n",
    "                #start to maintence\n",
    "                #Neuron h1\n",
    "                self.h1.weights[0] -= learn_rate*d_L_d_ypred*d_ypred_d_h1*d_h1_d_h1w1\n",
    "                self.h1.weights[1] -= learn_rate*d_L_d_ypred*d_ypred_d_h1*d_h1_d_h1w2\n",
    "                self.h1.bias -= learn_rate*d_L_d_ypred*d_ypred_d_h1*d_h1_d_b1\n",
    "                \n",
    "                #Neuron h2\n",
    "                self.h2.weights[0] -= learn_rate*d_L_d_ypred*d_ypred_d_h2*d_h2_d_h2w1\n",
    "                self.h2.weights[1] -= learn_rate*d_L_d_ypred*d_ypred_d_h2*d_h2_d_h2w2\n",
    "                self.h2.bias -= learn_rate*d_L_d_ypred*d_ypred_d_h2*d_h2_d_b2\n",
    "                \n",
    "                #Neuron o1\n",
    "                self.o1.weights[0] -= learn_rate*d_L_d_ypred*d_ypred_d_o1w1\n",
    "                self.o1.weights[1] -= learn_rate*d_L_d_ypred*d_ypred_d_o1w2\n",
    "                self.o1.bias -= learn_rate*d_L_d_ypred*d_ypred_d_b3\n",
    "                \n",
    "                if epoch % 10 == 0:\n",
    "                    y_pred = np.apply_along_axis(self.feedforward,1,data)\n",
    "                    # with dimension = 1 in col way to get function feedforward used in data Dataset\n",
    "                    print(y_pred)\n",
    "                    loss = mseLoss(all_y_trues,y_pred)\n",
    "                    print(\"Epoch: %d loss: %.3f\" % (epoch,loss))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5093739 0.5093739 0.5093739 0.5093739]\n",
      "Epoch: 0 loss: 0.250\n",
      "[0.49982747 0.49982911 0.49982862 0.49982666]\n",
      "Epoch: 0 loss: 0.250\n",
      "[0.49050611 0.49059412 0.4905681  0.49046254]\n",
      "Epoch: 0 loss: 0.250\n",
      "[0.50011344 0.5001123  0.50011264 0.50011398]\n",
      "Epoch: 0 loss: 0.250\n",
      "[0.52007048 0.51188347 0.51399716 0.52432127]\n",
      "Epoch: 10 loss: 0.246\n",
      "[0.51218655 0.50400559 0.50602953 0.51649081]\n",
      "Epoch: 10 loss: 0.245\n",
      "[0.50400504 0.4965219  0.49831486 0.50797972]\n",
      "Epoch: 10 loss: 0.246\n",
      "[0.51485914 0.50390787 0.50646537 0.52071584]\n",
      "Epoch: 10 loss: 0.244\n",
      "[0.58761257 0.49055101 0.49417422 0.64138737]\n",
      "Epoch: 20 loss: 0.196\n",
      "[0.58170508 0.48440206 0.48796443 0.63575759]\n",
      "Epoch: 20 loss: 0.195\n",
      "[0.57584909 0.47823958 0.48160163 0.63008086]\n",
      "Epoch: 20 loss: 0.194\n",
      "[0.58529573 0.48252739 0.48591688 0.641766  ]\n",
      "Epoch: 20 loss: 0.192\n",
      "[0.64172108 0.45430348 0.45589809 0.72376754]\n",
      "Epoch: 30 loss: 0.155\n",
      "[0.63654541 0.44872439 0.45031074 0.71924221]\n",
      "Epoch: 30 loss: 0.154\n",
      "[0.63150389 0.44321085 0.44475767 0.71470593]\n",
      "Epoch: 30 loss: 0.153\n",
      "[0.63787558 0.44608337 0.44764035 0.72172008]\n",
      "Epoch: 30 loss: 0.152\n",
      "[0.6782347  0.41276614 0.41347975 0.77053129]\n",
      "Epoch: 40 loss: 0.124\n",
      "[0.67385811 0.40792446 0.40863503 0.7669739 ]\n",
      "Epoch: 40 loss: 0.124\n",
      "[0.66957312 0.4031628  0.40386428 0.7634182 ]\n",
      "Epoch: 40 loss: 0.123\n",
      "[0.67413993 0.40522093 0.40592765 0.76800421]\n",
      "Epoch: 40 loss: 0.122\n",
      "[0.70728677 0.37357796 0.37389998 0.80105819]\n",
      "Epoch: 50 loss: 0.101\n",
      "[0.70365538 0.36949535 0.36981586 0.798257  ]\n",
      "Epoch: 50 loss: 0.100\n",
      "[0.70008015 0.36548866 0.36580654 0.79546374]\n",
      "Epoch: 50 loss: 0.100\n",
      "[0.70352943 0.36703357 0.36735365 0.79868928]\n",
      "Epoch: 50 loss: 0.100\n",
      "[0.73223619 0.33921097 0.33936043 0.82308002]\n",
      "Epoch: 60 loss: 0.083\n",
      "[0.72924462 0.33581083 0.33595955 0.82085473]\n",
      "Epoch: 60 loss: 0.083\n",
      "[0.72628853 0.33247572 0.33262347 0.81863865]\n",
      "Epoch: 60 loss: 0.082\n",
      "[0.72898221 0.333672   0.33382064 0.82102368]\n",
      "Epoch: 60 loss: 0.082\n",
      "[0.75393559 0.30987229 0.30994525 0.83990901]\n",
      "Epoch: 70 loss: 0.070\n",
      "[0.75146874 0.30704521 0.3071178  0.83811889]\n",
      "Epoch: 70 loss: 0.069\n",
      "[0.74902567 0.30427121 0.30434338 0.83633721]\n",
      "Epoch: 70 loss: 0.069\n",
      "[0.75118042 0.30522049 0.30529302 0.8381682 ]\n",
      "Epoch: 70 loss: 0.069\n",
      "[0.77272232 0.28501562 0.28505351 0.85323736]\n",
      "Epoch: 80 loss: 0.059\n",
      "[0.77067584 0.28265438 0.28269207 0.85177677]\n",
      "Epoch: 80 loss: 0.059\n",
      "[0.76864616 0.28033559 0.28037308 0.85032327]\n",
      "Epoch: 80 loss: 0.058\n",
      "[0.77040403 0.28110488 0.28114254 0.851772  ]\n",
      "Epoch: 80 loss: 0.058\n",
      "[0.7889251  0.26393249 0.26395346 0.86406708]\n",
      "Epoch: 90 loss: 0.051\n",
      "[0.78721238 0.26194507 0.26196594 0.86285808]\n",
      "Epoch: 90 loss: 0.050\n",
      "[0.78551201 0.25999143 0.26001219 0.8616549 ]\n",
      "Epoch: 90 loss: 0.050\n",
      "[0.78697084 0.26062651 0.26064735 0.86283013]\n",
      "Epoch: 90 loss: 0.050\n",
      "[0.80290292 0.24595641 0.24596873 0.87304764]\n",
      "Epoch: 100 loss: 0.044\n",
      "[0.80145521 0.24426835 0.24428061 0.87203303]\n",
      "Epoch: 100 loss: 0.044\n",
      "[0.80001685 0.24260723 0.24261944 0.87102314]\n",
      "Epoch: 100 loss: 0.044\n",
      "[0.80124594 0.24314011 0.24315236 0.87199662]\n",
      "Epoch: 100 loss: 0.044\n",
      "[0.81500399 0.23052047 0.23052811 0.88062252]\n",
      "Epoch: 110 loss: 0.039\n",
      "[0.8137678  0.22907305 0.22908066 0.87976011]\n",
      "Epoch: 110 loss: 0.039\n",
      "[0.81253886 0.22764728 0.22765486 0.87890155]\n",
      "Epoch: 110 loss: 0.038\n",
      "[0.81358835 0.22810084 0.22810843 0.87972221]\n",
      "Epoch: 110 loss: 0.038\n",
      "[0.82553766 0.21716205 0.21716702 0.88710496]\n",
      "Epoch: 120 loss: 0.034\n",
      "[0.82447167 0.21590944 0.21591438 0.88636337]\n",
      "Epoch: 120 loss: 0.034\n",
      "[0.82341141 0.21467437 0.21467929 0.88562492]\n",
      "Epoch: 120 loss: 0.034\n",
      "[0.82431822 0.21506526 0.2150702  0.88632714]\n",
      "Epoch: 120 loss: 0.034\n",
      "[0.83476552 0.20551015 0.20551351 0.89272205]\n",
      "Epoch: 130 loss: 0.031\n",
      "[0.83383778 0.20441658 0.20441993 0.89207765]\n",
      "Epoch: 130 loss: 0.031\n",
      "[0.83291464 0.20333736 0.2033407  0.89143583]\n",
      "Epoch: 130 loss: 0.031\n",
      "[0.83370641 0.20367797 0.20368132 0.8920444 ]\n",
      "Epoch: 130 loss: 0.031\n",
      "[0.84290328 0.19526896 0.19527132 0.89764208]\n",
      "Epoch: 140 loss: 0.028\n",
      "[0.84208894 0.19430642 0.19430877 0.89707684]\n",
      "Epoch: 140 loss: 0.028\n",
      "[0.84127834 0.19335574 0.19335809 0.89651375]\n",
      "Epoch: 140 loss: 0.028\n",
      "[0.84197611 0.19365541 0.19365776 0.89704698]\n",
      "Epoch: 140 loss: 0.028\n",
      "[0.8501268  0.18620253 0.18620424 0.90199223]\n",
      "Epoch: 150 loss: 0.025\n",
      "[0.84940639 0.18534894 0.18535065 0.90149224]\n",
      "Epoch: 150 loss: 0.025\n",
      "[0.84868905 0.18450525 0.18450695 0.90099404]\n",
      "Epoch: 150 loss: 0.025\n",
      "[0.84930908 0.18477117 0.18477287 0.90146573]\n",
      "Epoch: 150 loss: 0.025\n",
      "[0.85657903 0.17812194 0.17812321 0.90587034]\n",
      "Epoch: 160 loss: 0.023\n",
      "[0.85593715 0.17735974 0.17736101 0.9054247 ]\n",
      "Epoch: 160 loss: 0.023\n",
      "[0.85529782 0.17660588 0.17660714 0.90498058]\n",
      "Epoch: 160 loss: 0.023\n",
      "[0.85585283 0.17684363 0.1768449  0.90540131]\n",
      "Epoch: 160 loss: 0.023\n",
      "[0.86237632 0.170875   0.17087596 0.90935283]\n",
      "Epoch: 170 loss: 0.021\n",
      "[0.86180067 0.17019012 0.17019108 0.90895293]\n",
      "Epoch: 170 loss: 0.021\n",
      "[0.86122718 0.16951231 0.16951327 0.90855432]\n",
      "Epoch: 170 loss: 0.021\n",
      "[0.86172726 0.16972633 0.1697273  0.90893236]\n",
      "Epoch: 170 loss: 0.021\n",
      "[0.86761374 0.16433829 0.16433904 0.9125003 ]\n",
      "Epoch: 180 loss: 0.020\n",
      "[0.86709443 0.16371935 0.1637201  0.91213924]\n",
      "Epoch: 180 loss: 0.020\n",
      "[0.86657695 0.16310646 0.1631072  0.91177928]\n",
      "Epoch: 180 loss: 0.020\n",
      "[0.8670302  0.16330028 0.16330103 0.91212117]\n",
      "Epoch: 180 loss: 0.020\n",
      "[0.87236942 0.158411   0.15841159 0.9153614 ]\n",
      "Epoch: 190 loss: 0.018\n",
      "[0.8718984  0.15784871 0.1578493  0.91503359]\n",
      "Epoch: 190 loss: 0.018\n",
      "[0.87142895 0.15729164 0.15729223 0.91470674]\n",
      "Epoch: 190 loss: 0.018\n",
      "[0.87184195 0.15746813 0.15746872 0.91501772]\n",
      "Epoch: 190 loss: 0.018\n",
      "[0.87670794 0.15301018 0.15301065 0.91797564]\n",
      "Epoch: 200 loss: 0.017\n",
      "[0.87627861 0.1524969  0.15249738 0.91767653]\n",
      "Epoch: 200 loss: 0.017\n",
      "[0.87585064 0.15198815 0.15198863 0.91737824]\n",
      "Epoch: 200 loss: 0.017\n",
      "[0.87622878 0.15214965 0.15215013 0.91766259]\n",
      "Epoch: 200 loss: 0.017\n",
      "[0.88068304 0.14806704 0.14806743 0.92037548]\n",
      "Epoch: 210 loss: 0.016\n",
      "[0.88028995 0.14759645 0.14759684 0.92010129]\n",
      "Epoch: 210 loss: 0.016\n",
      "[0.87989804 0.14712981 0.1471302  0.91982784]\n",
      "Epoch: 210 loss: 0.016\n",
      "[0.88024577 0.14727825 0.14727864 0.92008905]\n",
      "Epoch: 210 loss: 0.016\n",
      "[0.88433973 0.1435242  0.14352452 0.9225878 ]\n",
      "Epoch: 220 loss: 0.015\n",
      "[0.88397833 0.14309099 0.14309132 0.92233542]\n",
      "Epoch: 220 loss: 0.015\n",
      "[0.88361796 0.14266127 0.14266159 0.92208368]\n",
      "Epoch: 220 loss: 0.015\n",
      "[0.88393901 0.14279826 0.14279858 0.92232465]\n",
      "Epoch: 220 loss: 0.015\n",
      "[0.88771592 0.13933338 0.13933365 0.9246351 ]\n",
      "Epoch: 230 loss: 0.014\n",
      "[0.8873824  0.13893312 0.13893339 0.9244019 ]\n",
      "Epoch: 230 loss: 0.014\n",
      "[0.88704977 0.13853593 0.1385362  0.92416927]\n",
      "Epoch: 230 loss: 0.014\n",
      "[0.88734728 0.13866282 0.13866309 0.92439242]\n",
      "Epoch: 230 loss: 0.014\n",
      "[0.89084379 0.13545378 0.13545401 0.92653634]\n",
      "Epoch: 240 loss: 0.014\n",
      "[0.89053491 0.13508269 0.13508292 0.92632011]\n",
      "Epoch: 240 loss: 0.013\n",
      "[0.89022683 0.13471433 0.13471455 0.92610438]\n",
      "Epoch: 240 loss: 0.013\n",
      "[0.89050343 0.13483227 0.13483249 0.92631176]\n",
      "Epoch: 240 loss: 0.013\n",
      "[0.89375075 0.13185065 0.13185084 0.92830761]\n",
      "Epoch: 250 loss: 0.013\n",
      "[0.89346378 0.13150552 0.13150571 0.92810647]\n",
      "Epoch: 250 loss: 0.013\n",
      "[0.89317751 0.13116282 0.13116301 0.92790577]\n",
      "Epoch: 250 loss: 0.013\n",
      "[0.89343547 0.13127278 0.13127297 0.92809911]\n",
      "Epoch: 250 loss: 0.013\n",
      "[0.89646035 0.12849424 0.12849441 0.92996269]\n",
      "Epoch: 260 loss: 0.012\n",
      "[0.89619292 0.12817231 0.12817248 0.92977501]\n",
      "Epoch: 260 loss: 0.012\n",
      "[0.89592613 0.12785256 0.12785273 0.92958775]\n",
      "Epoch: 260 loss: 0.012\n",
      "[0.8961674  0.12795538 0.12795554 0.92976853]\n",
      "Epoch: 260 loss: 0.012\n",
      "[0.89899286 0.12535895 0.12535909 0.9315134 ]\n",
      "Epoch: 270 loss: 0.012\n",
      "[0.89874296 0.12505785 0.125058   0.93133782]\n",
      "Epoch: 270 loss: 0.012\n",
      "[0.89849364 0.12475871 0.12475885 0.93116261]\n",
      "Epoch: 270 loss: 0.012\n",
      "[0.89871988 0.1248551  0.12485525 0.93133211]\n",
      "Epoch: 270 loss: 0.012\n",
      "[0.90136586 0.12242262 0.12242275 0.93297001]\n",
      "Epoch: 280 loss: 0.011\n",
      "[0.90113175 0.12214029 0.12214042 0.93280532]\n",
      "Epoch: 280 loss: 0.011\n",
      "[0.90089815 0.12185973 0.12185986 0.93264096]\n",
      "Epoch: 280 loss: 0.011\n",
      "[0.90111082 0.12195033 0.12195045 0.93280029]\n",
      "Epoch: 280 loss: 0.011\n",
      "[0.90359468 0.11966598 0.11966609 0.93434142]\n",
      "Epoch: 290 loss: 0.011\n",
      "[0.90337483 0.11940063 0.11940074 0.93418657]\n",
      "Epoch: 290 loss: 0.011\n",
      "[0.90315544 0.11913688 0.11913699 0.93403203]\n",
      "Epoch: 290 loss: 0.011\n",
      "[0.9033558  0.11922222 0.11922233 0.93418215]\n",
      "Epoch: 290 loss: 0.011\n",
      "[0.9056927  0.1170722  0.1170723  0.93563543]\n",
      "Epoch: 300 loss: 0.010\n",
      "[0.90548578 0.11682226 0.11682236 0.93548952]\n",
      "Epoch: 300 loss: 0.010\n",
      "[0.90527927 0.11657377 0.11657387 0.93534389]\n",
      "Epoch: 300 loss: 0.010\n",
      "[0.90546844 0.11665433 0.11665443 0.93548562]\n",
      "Epoch: 300 loss: 0.010\n",
      "[0.9076717  0.11462651 0.11462659 0.93685888]\n",
      "Epoch: 310 loss: 0.010\n",
      "[0.90747653 0.1143906  0.11439068 0.93672111]\n",
      "Epoch: 310 loss: 0.010\n",
      "[0.90728175 0.11415601 0.1141561  0.93658359]\n",
      "Epoch: 310 loss: 0.010\n",
      "[0.9074607  0.11423221 0.1142323  0.93671769]\n",
      "Epoch: 310 loss: 0.010\n",
      "[0.90954204 0.11231589 0.11231597 0.93801783]\n",
      "Epoch: 320 loss: 0.009\n",
      "[0.90935761 0.11209279 0.11209287 0.93788749]\n",
      "Epoch: 320 loss: 0.009\n",
      "[0.90917352 0.11187091 0.11187099 0.93775738]\n",
      "Epoch: 320 loss: 0.009\n",
      "[0.90934313 0.11194312 0.1119432  0.93788449]\n",
      "Epoch: 320 loss: 0.009\n",
      "[0.91131292 0.11012883 0.1101289  0.93911761]\n",
      "Epoch: 330 loss: 0.009\n",
      "[0.91113831 0.10991747 0.10991754 0.93899408]\n",
      "Epoch: 330 loss: 0.009\n",
      "[0.91096402 0.10970723 0.1097073  0.93887075]\n",
      "Epoch: 330 loss: 0.009\n",
      "[0.91112504 0.10977578 0.10977585 0.93899145]\n",
      "Epoch: 330 loss: 0.009\n",
      "[0.91299251 0.10805512 0.10805518 0.94016299]\n",
      "Epoch: 340 loss: 0.009\n",
      "[0.91282691 0.10785454 0.1078546  0.94004571]\n",
      "Epoch: 340 loss: 0.009\n",
      "[0.9126616  0.10765498 0.10765505 0.93992862]\n",
      "Epoch: 340 loss: 0.009\n",
      "[0.91281473 0.10772017 0.10772023 0.94004343]\n",
      "Epoch: 340 loss: 0.009\n",
      "[0.91458808 0.10608565 0.1060857  0.94115822]\n",
      "Epoch: 350 loss: 0.008\n",
      "[0.91443078 0.10589499 0.10589505 0.94104669]\n",
      "Epoch: 350 loss: 0.008\n",
      "[0.91427374 0.10570528 0.10570534 0.94093535]\n",
      "Epoch: 350 loss: 0.008\n",
      "[0.91441958 0.10576736 0.10576742 0.94104471]\n",
      "Epoch: 350 loss: 0.008\n",
      "[0.91610616 0.10421228 0.10421233 0.9421071 ]\n",
      "Epoch: 360 loss: 0.008\n",
      "[0.9159565  0.10403079 0.10403084 0.94200088]\n",
      "Epoch: 360 loss: 0.008\n",
      "[0.9158071  0.10385016 0.10385021 0.94189484]\n",
      "Epoch: 360 loss: 0.008\n",
      "[0.91594619 0.10390937 0.10390942 0.94199918]\n",
      "Epoch: 360 loss: 0.008\n",
      "[0.91755259 0.10242772 0.10242776 0.94301304]\n",
      "Epoch: 370 loss: 0.008\n",
      "[0.91741    0.1022547  0.10225474 0.94291175]\n",
      "Epoch: 370 loss: 0.008\n",
      "[0.91726764 0.10208249 0.10208253 0.9428106 ]\n",
      "Epoch: 370 loss: 0.008\n",
      "[0.91740049 0.10213903 0.10213907 0.94291028]\n",
      "Epoch: 370 loss: 0.008\n",
      "[0.91893263 0.1007254  0.10072544 0.94387913]\n",
      "Epoch: 380 loss: 0.008\n",
      "[0.91879659 0.10056024 0.10056028 0.9437824 ]\n",
      "Epoch: 380 loss: 0.007\n",
      "[0.91866076 0.10039582 0.10039586 0.9436858 ]\n",
      "Epoch: 380 loss: 0.007\n",
      "[0.91878781 0.10044989 0.10044993 0.94378115]\n",
      "Epoch: 380 loss: 0.007\n",
      "[0.92025103 0.09909941 0.09909945 0.94470815]\n",
      "Epoch: 390 loss: 0.007\n",
      "[0.92012107 0.09894154 0.09894158 0.94461565]\n",
      "Epoch: 390 loss: 0.007\n",
      "[0.91999131 0.09878437 0.09878441 0.94452328]\n",
      "Epoch: 390 loss: 0.007\n",
      "[0.92011295 0.09883614 0.09883618 0.94461461]\n",
      "Epoch: 390 loss: 0.007\n",
      "[0.92151208 0.09754439 0.09754443 0.94550263]\n",
      "Epoch: 400 loss: 0.007\n",
      "[0.92138778 0.09739332 0.09739335 0.94541407]\n",
      "Epoch: 400 loss: 0.007\n",
      "[0.92126366 0.09724289 0.09724293 0.94532563]\n",
      "Epoch: 400 loss: 0.007\n",
      "[0.92138026 0.09729252 0.09729255 0.9454132 ]\n",
      "Epoch: 400 loss: 0.007\n",
      "[0.92271969 0.0960555  0.09605553 0.94626484]\n",
      "Epoch: 410 loss: 0.007\n",
      "[0.92260066 0.09591076 0.09591079 0.94617996]\n",
      "Epoch: 410 loss: 0.007\n",
      "[0.9224818  0.09576662 0.09576666 0.9460952 ]\n",
      "Epoch: 410 loss: 0.007\n",
      "[0.9225937  0.09581425 0.09581428 0.94617925]\n",
      "Epoch: 410 loss: 0.007\n",
      "[0.9238774  0.09462832 0.09462835 0.94699688]\n",
      "Epoch: 420 loss: 0.007\n",
      "[0.92376329 0.09448949 0.09448952 0.94691544]\n",
      "Epoch: 420 loss: 0.007\n",
      "[0.92364934 0.09435124 0.09435127 0.9468341 ]\n",
      "Epoch: 420 loss: 0.007\n",
      "[0.92375684 0.09439698 0.09439701 0.94691488]\n",
      "Epoch: 420 loss: 0.007\n",
      "[0.92498844 0.09325883 0.09325886 0.94770066]\n",
      "Epoch: 430 loss: 0.006\n",
      "[0.92487893 0.09312554 0.09312557 0.94762243]\n",
      "Epoch: 430 loss: 0.006\n",
      "[0.92476958 0.09299279 0.09299281 0.9475443 ]\n",
      "Epoch: 430 loss: 0.006\n",
      "[0.92487294 0.09303678 0.09303681 0.947622  ]\n",
      "Epoch: 430 loss: 0.006\n",
      "[0.92605577 0.09194337 0.0919434  0.94837791]\n",
      "Epoch: 440 loss: 0.006\n",
      "[0.92595057 0.09181527 0.0918153  0.94830269]\n",
      "Epoch: 440 loss: 0.006\n",
      "[0.92584552 0.09168767 0.0916877  0.94822758]\n",
      "Epoch: 440 loss: 0.006\n",
      "[0.92594501 0.09173001 0.09173004 0.94830239]\n",
      "Epoch: 440 loss: 0.006\n",
      "[0.92708209 0.09067858 0.09067861 0.94903023]\n",
      "Epoch: 450 loss: 0.006\n",
      "[0.92698093 0.09055535 0.09055538 0.94895785]\n",
      "Epoch: 450 loss: 0.006\n",
      "[0.92687991 0.09043259 0.09043262 0.94888555]\n",
      "Epoch: 450 loss: 0.006\n",
      "[0.92697576 0.09047339 0.09047341 0.94895765]\n",
      "Epoch: 450 loss: 0.006\n",
      "[0.92806987 0.08946139 0.08946141 0.94965909]\n",
      "Epoch: 460 loss: 0.006\n",
      "[0.92797251 0.08934274 0.08934276 0.94958937]\n",
      "Epoch: 460 loss: 0.006\n",
      "[0.92787528 0.08922453 0.08922455 0.94951973]\n",
      "Epoch: 460 loss: 0.006\n",
      "[0.9279677  0.08926386 0.08926388 0.94958927]\n",
      "Epoch: 460 loss: 0.006\n",
      "[0.9290214  0.08828897 0.08828899 0.95026584]\n",
      "Epoch: 470 loss: 0.006\n",
      "[0.92892761 0.08817462 0.08817464 0.95019863]\n",
      "Epoch: 470 loss: 0.006\n",
      "[0.92883395 0.0880607  0.08806072 0.95013149]\n",
      "Epoch: 470 loss: 0.006\n",
      "[0.92892314 0.08809865 0.08809867 0.95019862]\n",
      "Epoch: 470 loss: 0.006\n",
      "[0.92993876 0.08715871 0.08715873 0.95085173]\n",
      "Epoch: 480 loss: 0.006\n",
      "[0.92984835 0.08704843 0.08704845 0.95078688]\n",
      "Epoch: 480 loss: 0.006\n",
      "[0.92975805 0.08693854 0.08693856 0.9507221 ]\n",
      "Epoch: 480 loss: 0.006\n",
      "[0.92984418 0.0869752  0.08697521 0.95078695]\n",
      "Epoch: 480 loss: 0.006\n",
      "[0.93082391 0.08606822 0.08606824 0.9514179 ]\n",
      "Epoch: 490 loss: 0.005\n",
      "[0.93073667 0.08596177 0.08596179 0.95135528]\n",
      "Epoch: 490 loss: 0.005\n",
      "[0.93064954 0.08585569 0.08585571 0.95129273]\n",
      "Epoch: 490 loss: 0.005\n",
      "[0.93073279 0.08589112 0.08589114 0.95135543]\n",
      "Epoch: 490 loss: 0.005\n",
      "[0.93167862 0.08501528 0.0850153  0.95196543]\n",
      "Epoch: 500 loss: 0.005\n",
      "[0.93159438 0.08491245 0.08491247 0.95190491]\n",
      "Epoch: 500 loss: 0.005\n",
      "[0.93151024 0.08480998 0.08480999 0.95184446]\n",
      "Epoch: 500 loss: 0.005\n",
      "[0.93159077 0.08484425 0.08484426 0.95190513]\n",
      "Epoch: 500 loss: 0.005\n",
      "[0.93250455 0.08399785 0.08399787 0.95249529]\n",
      "Epoch: 510 loss: 0.005\n",
      "[0.93242315 0.08389845 0.08389846 0.95243676]\n",
      "Epoch: 510 loss: 0.005\n",
      "[0.93234184 0.08379938 0.08379939 0.95237831]\n",
      "Epoch: 510 loss: 0.005\n",
      "[0.93241978 0.08383255 0.08383256 0.95243705]\n",
      "Epoch: 510 loss: 0.005\n",
      "[0.93330324 0.08301402 0.08301404 0.9530084 ]\n",
      "Epoch: 520 loss: 0.005\n",
      "[0.93322452 0.08291787 0.08291788 0.95295177]\n",
      "Epoch: 520 loss: 0.005\n",
      "[0.93314589 0.08282202 0.08282204 0.95289519]\n",
      "Epoch: 520 loss: 0.005\n",
      "[0.93322138 0.08285415 0.08285417 0.9529521 ]\n",
      "Epoch: 520 loss: 0.005\n",
      "[0.9340761  0.08206204 0.08206205 0.95350561]\n",
      "Epoch: 530 loss: 0.005\n",
      "[0.93399993 0.08196896 0.08196897 0.95345077]\n",
      "Epoch: 530 loss: 0.005\n",
      "[0.93392384 0.08187618 0.08187619 0.95339599]\n",
      "Epoch: 530 loss: 0.005\n",
      "[0.93399699 0.08190731 0.08190733 0.95345116]\n",
      "Epoch: 530 loss: 0.005\n",
      "[0.93482446 0.08114026 0.08114028 0.95398772]\n",
      "Epoch: 540 loss: 0.005\n",
      "[0.9347507  0.0810501  0.08105012 0.95393458]\n",
      "Epoch: 540 loss: 0.005\n",
      "[0.93467702 0.08096022 0.08096024 0.9538815 ]\n",
      "Epoch: 540 loss: 0.005\n",
      "[0.93474797 0.08099042 0.08099043 0.95393502]\n",
      "Epoch: 540 loss: 0.005\n",
      "[0.93554955 0.08024717 0.08024718 0.95445547]\n",
      "Epoch: 550 loss: 0.005\n",
      "[0.93547809 0.08015978 0.08015979 0.95440395]\n",
      "Epoch: 550 loss: 0.005\n",
      "[0.9354067  0.08007266 0.08007268 0.95435248]\n",
      "Epoch: 550 loss: 0.005\n",
      "[0.93547554 0.08010197 0.08010198 0.95440443]\n",
      "Epoch: 550 loss: 0.005\n",
      "[0.93625252 0.07938134 0.07938135 0.95490954]\n",
      "Epoch: 560 loss: 0.005\n",
      "[0.93618324 0.07929659 0.0792966  0.95485956]\n",
      "Epoch: 560 loss: 0.005\n",
      "[0.93611403 0.07921209 0.07921211 0.95480962]\n",
      "Epoch: 560 loss: 0.005\n",
      "[0.93618086 0.07924054 0.07924056 0.95486008]\n",
      "Epoch: 560 loss: 0.005\n",
      "[0.93693444 0.07854144 0.07854145 0.95535058]\n",
      "Epoch: 570 loss: 0.005\n",
      "[0.93686724 0.0784592  0.07845922 0.95530206]\n",
      "Epoch: 570 loss: 0.005\n",
      "[0.9368001  0.07837721 0.07837722 0.95525359]\n",
      "Epoch: 570 loss: 0.005\n",
      "[0.93686502 0.07840485 0.07840486 0.95530261]\n",
      "Epoch: 570 loss: 0.005\n",
      "[0.93759632 0.07772625 0.07772626 0.95577919]\n",
      "Epoch: 580 loss: 0.004\n",
      "[0.93753109 0.0776464  0.07764641 0.95573207]\n",
      "Epoch: 580 loss: 0.004\n",
      "[0.93746592 0.07756679 0.0775668  0.95568499]\n",
      "Epoch: 580 loss: 0.004\n",
      "[0.93752902 0.07759365 0.07759366 0.95573265]\n",
      "Epoch: 580 loss: 0.004\n",
      "[0.93823909 0.07693459 0.0769346  0.95619594]\n",
      "Epoch: 590 loss: 0.004\n",
      "[0.93817574 0.07685703 0.07685704 0.95615015]\n",
      "Epoch: 590 loss: 0.004\n",
      "[0.93811246 0.07677968 0.07677969 0.9561044 ]\n",
      "Epoch: 590 loss: 0.004\n",
      "[0.93817382 0.07680581 0.07680582 0.95615076]\n",
      "Epoch: 590 loss: 0.004\n",
      "[0.93886363 0.07616539 0.0761654  0.95660135]\n",
      "Epoch: 600 loss: 0.004\n",
      "[0.93880208 0.07609001 0.07609002 0.95655683]\n",
      "Epoch: 600 loss: 0.004\n",
      "[0.93874059 0.07601483 0.07601484 0.95651235]\n",
      "Epoch: 600 loss: 0.004\n",
      "[0.93880028 0.07604024 0.07604025 0.95655748]\n",
      "Epoch: 600 loss: 0.004\n",
      "[0.93947076 0.07541763 0.07541764 0.95699594]\n",
      "Epoch: 610 loss: 0.004\n",
      "[0.93941093 0.07534433 0.07534433 0.95695263]\n",
      "Epoch: 610 loss: 0.004\n",
      "[0.93935115 0.07527122 0.07527123 0.95690936]\n",
      "Epoch: 610 loss: 0.004\n",
      "[0.93940925 0.07529596 0.07529597 0.9569533 ]\n",
      "Epoch: 610 loss: 0.004\n",
      "[0.94006126 0.07469036 0.07469037 0.95738015]\n",
      "Epoch: 620 loss: 0.004\n",
      "[0.94000307 0.07461904 0.07461905 0.95733801]\n",
      "Epoch: 620 loss: 0.004\n",
      "[0.93994492 0.07454791 0.07454792 0.9572959 ]\n",
      "Epoch: 620 loss: 0.004\n",
      "[0.94000151 0.074572   0.07457201 0.9573387 ]\n",
      "Epoch: 620 loss: 0.004\n",
      "[0.94063586 0.07398267 0.07398268 0.95775444]\n",
      "Epoch: 630 loss: 0.004\n",
      "[0.94057923 0.07391325 0.07391326 0.95771341]\n",
      "Epoch: 630 loss: 0.004\n",
      "[0.94052265 0.07384402 0.07384403 0.95767241]\n",
      "Epoch: 630 loss: 0.004\n",
      "[0.94057778 0.07386749 0.0738675  0.95771412]\n",
      "Epoch: 630 loss: 0.004\n",
      "[0.94119522 0.07329372 0.07329373 0.95811922]\n",
      "Epoch: 640 loss: 0.004\n",
      "[0.94114009 0.07322613 0.07322614 0.95807925]\n",
      "Epoch: 640 loss: 0.004\n",
      "[0.94108501 0.07315871 0.07315872 0.95803931]\n",
      "Epoch: 640 loss: 0.004\n",
      "[0.94113875 0.07318159 0.07318159 0.95807998]\n",
      "Epoch: 640 loss: 0.004\n",
      "[0.94174001 0.07262273 0.07262274 0.95847488]\n",
      "Epoch: 650 loss: 0.004\n",
      "[0.94168632 0.07255688 0.07255688 0.95843593]\n",
      "Epoch: 650 loss: 0.004\n",
      "[0.94163267 0.07249119 0.0724912  0.95839701]\n",
      "Epoch: 650 loss: 0.004\n",
      "[0.94168507 0.0725135  0.07251351 0.95843667]\n",
      "Epoch: 650 loss: 0.004\n",
      "[0.94227083 0.07196893 0.07196894 0.95882179]\n",
      "Epoch: 660 loss: 0.004\n",
      "[0.94221851 0.07190475 0.07190476 0.95878381]\n",
      "Epoch: 660 loss: 0.004\n",
      "[0.94216624 0.07184073 0.07184074 0.95874587]\n",
      "Epoch: 660 loss: 0.004\n",
      "[0.94221735 0.0718625  0.0718625  0.95878457]\n",
      "Epoch: 660 loss: 0.004\n",
      "[0.94278825 0.07133163 0.07133164 0.95916029]\n",
      "Epoch: 670 loss: 0.004\n",
      "[0.94273725 0.07126906 0.07126906 0.95912326]\n",
      "Epoch: 670 loss: 0.004\n",
      "[0.9426863  0.07120663 0.07120664 0.95908625]\n",
      "Epoch: 670 loss: 0.004\n",
      "[0.94273618 0.07122787 0.07122788 0.95912403]\n",
      "Epoch: 670 loss: 0.004\n",
      "[0.94329281 0.07071016 0.07071017 0.95949073]\n",
      "Epoch: 680 loss: 0.004\n",
      "[0.94324308 0.07064912 0.07064913 0.95945459]\n",
      "Epoch: 680 loss: 0.004\n",
      "[0.94319339 0.07058823 0.07058824 0.95941849]\n",
      "Epoch: 680 loss: 0.004\n",
      "[0.94324208 0.07060897 0.07060897 0.95945538]\n",
      "Epoch: 680 loss: 0.004\n",
      "[0.94378502 0.07010388 0.07010389 0.9598134 ]\n",
      "Epoch: 690 loss: 0.004\n",
      "[0.94373651 0.07004432 0.07004433 0.95977813]\n",
      "Epoch: 690 loss: 0.004\n",
      "[0.94368804 0.06998491 0.06998491 0.95974289]\n",
      "Epoch: 690 loss: 0.004\n",
      "[0.94373559 0.07000515 0.07000516 0.95977893]\n",
      "Epoch: 690 loss: 0.004\n",
      "[0.94426537 0.0695122  0.06951221 0.96012861]\n",
      "Epoch: 700 loss: 0.004\n",
      "[0.94421802 0.06945406 0.06945407 0.96009418]\n",
      "Epoch: 700 loss: 0.004\n",
      "[0.94417072 0.06939606 0.06939607 0.96005976]\n",
      "Epoch: 700 loss: 0.004\n",
      "[0.94421717 0.06941585 0.06941585 0.96009499]\n",
      "Epoch: 700 loss: 0.004\n",
      "[0.94473431 0.06893456 0.06893457 0.96043664]\n",
      "Epoch: 710 loss: 0.004\n",
      "[0.94468809 0.06887779 0.0688778  0.960403  ]\n",
      "Epoch: 710 loss: 0.004\n",
      "[0.94464191 0.06882115 0.06882116 0.96036939]\n",
      "Epoch: 710 loss: 0.004\n",
      "[0.94468731 0.06884049 0.06884049 0.96040382]\n",
      "Epoch: 710 loss: 0.004\n",
      "[0.94519228 0.06837042 0.06837043 0.96073775]\n",
      "Epoch: 720 loss: 0.003\n",
      "[0.94514714 0.06831496 0.06831497 0.96070488]\n",
      "Epoch: 720 loss: 0.003\n",
      "[0.94510204 0.06825963 0.06825964 0.96067204]\n",
      "Epoch: 720 loss: 0.003\n",
      "[0.94514642 0.06827854 0.06827854 0.96070571]\n",
      "Epoch: 720 loss: 0.003\n",
      "[0.94563969 0.06781927 0.06781928 0.96103219]\n",
      "Epoch: 730 loss: 0.003\n",
      "[0.94559559 0.06776508 0.06776509 0.96100007]\n",
      "Epoch: 730 loss: 0.003\n",
      "[0.94555153 0.06771101 0.06771102 0.96096797]\n",
      "Epoch: 730 loss: 0.003\n",
      "[0.94559493 0.0677295  0.06772951 0.9610009 ]\n",
      "Epoch: 730 loss: 0.003\n",
      "[0.94607693 0.06728064 0.06728065 0.96132021]\n",
      "Epoch: 740 loss: 0.003\n",
      "[0.94603383 0.06722767 0.06722767 0.9612888 ]\n",
      "Epoch: 740 loss: 0.003\n",
      "[0.94599077 0.06717482 0.06717482 0.96125741]\n",
      "Epoch: 740 loss: 0.003\n",
      "[0.94603323 0.0671929  0.06719291 0.96128964]\n",
      "Epoch: 740 loss: 0.003\n",
      "[0.94650437 0.06675406 0.06675407 0.96160203]\n",
      "Epoch: 750 loss: 0.003\n",
      "[0.94646224 0.06670227 0.06670227 0.96157131]\n",
      "Epoch: 750 loss: 0.003\n",
      "[0.94642014 0.06665058 0.06665059 0.96154061]\n",
      "Epoch: 750 loss: 0.003\n",
      "[0.94646169 0.06666828 0.06666829 0.96157215]\n",
      "Epoch: 750 loss: 0.003\n",
      "[0.94692236 0.06623911 0.06623911 0.96187787]\n",
      "Epoch: 760 loss: 0.003\n",
      "[0.94688116 0.06618844 0.06618845 0.96184781]\n",
      "Epoch: 760 loss: 0.003\n",
      "[0.94683999 0.06613789 0.0661379  0.96181778]\n",
      "Epoch: 760 loss: 0.003\n",
      "[0.94688066 0.06615521 0.06615522 0.96184866]\n",
      "Epoch: 760 loss: 0.003\n",
      "[0.94733124 0.06573536 0.06573536 0.96214793]\n",
      "Epoch: 770 loss: 0.003\n",
      "[0.94729094 0.06568579 0.06568579 0.96211852]\n",
      "Epoch: 770 loss: 0.003\n",
      "[0.94725067 0.06563633 0.06563633 0.96208912]\n",
      "Epoch: 770 loss: 0.003\n",
      "[0.94729049 0.06565329 0.06565329 0.96211937]\n",
      "Epoch: 770 loss: 0.003\n",
      "[0.94773133 0.06524243 0.06524243 0.96241242]\n",
      "Epoch: 780 loss: 0.003\n",
      "[0.9476919  0.06519391 0.06519392 0.96238362]\n",
      "Epoch: 780 loss: 0.003\n",
      "[0.94765249 0.0651455  0.06514551 0.96235484]\n",
      "Epoch: 780 loss: 0.003\n",
      "[0.94769149 0.06516212 0.06516212 0.96238448]\n",
      "Epoch: 780 loss: 0.003\n",
      "[0.94812293 0.06475994 0.06475995 0.96267152]\n",
      "Epoch: 790 loss: 0.003\n",
      "[0.94808433 0.06471245 0.06471245 0.96264332]\n",
      "Epoch: 790 loss: 0.003\n",
      "[0.94804576 0.06466505 0.06466505 0.96261514]\n",
      "Epoch: 790 loss: 0.003\n",
      "[0.94808397 0.06468133 0.06468133 0.96264417]\n",
      "Epoch: 790 loss: 0.003\n",
      "[0.94850633 0.06428754 0.06428754 0.9629254 ]\n",
      "Epoch: 800 loss: 0.003\n",
      "[0.94846854 0.06424103 0.06424103 0.96289778]\n",
      "Epoch: 800 loss: 0.003\n",
      "[0.94843078 0.06419461 0.06419462 0.96287017]\n",
      "Epoch: 800 loss: 0.003\n",
      "[0.94846822 0.06421056 0.06421057 0.96289864]\n",
      "Epoch: 800 loss: 0.003\n",
      "[0.94888181 0.06382488 0.06382489 0.96317424]\n",
      "Epoch: 810 loss: 0.003\n",
      "[0.9488448  0.06377932 0.06377933 0.96314718]\n",
      "Epoch: 810 loss: 0.003\n",
      "[0.94880782 0.06373386 0.06373386 0.96312013]\n",
      "Epoch: 810 loss: 0.003\n",
      "[0.94884451 0.06374949 0.0637495  0.96314804]\n",
      "Epoch: 810 loss: 0.003\n",
      "[0.94924962 0.06337165 0.06337165 0.96341821]\n",
      "Epoch: 820 loss: 0.003\n",
      "[0.94921337 0.06332701 0.06332701 0.96339168]\n",
      "Epoch: 820 loss: 0.003\n",
      "[0.94917714 0.06328246 0.06328246 0.96336518]\n",
      "Epoch: 820 loss: 0.003\n",
      "[0.94921312 0.06329779 0.06329779 0.96339255]\n",
      "Epoch: 820 loss: 0.003\n",
      "[0.94961003 0.06292752 0.06292753 0.96365745]\n",
      "Epoch: 830 loss: 0.003\n",
      "[0.94957451 0.06288378 0.06288378 0.96363145]\n",
      "Epoch: 830 loss: 0.003\n",
      "[0.94953901 0.06284012 0.06284012 0.96360547]\n",
      "Epoch: 830 loss: 0.003\n",
      "[0.94957429 0.06285515 0.06285515 0.96363231]\n",
      "Epoch: 830 loss: 0.003\n",
      "[0.94996327 0.06249222 0.06249222 0.96389212]\n",
      "Epoch: 840 loss: 0.003\n",
      "[0.94992846 0.06244933 0.06244934 0.96386663]\n",
      "Epoch: 840 loss: 0.003\n",
      "[0.94989366 0.06240653 0.06240653 0.96384115]\n",
      "Epoch: 840 loss: 0.003\n",
      "[0.94992827 0.06242128 0.06242128 0.96386749]\n",
      "Epoch: 840 loss: 0.003\n",
      "[0.95030957 0.06206545 0.06206545 0.96412236]\n",
      "Epoch: 850 loss: 0.003\n",
      "[0.95027544 0.0620234  0.0620234  0.96409735]\n",
      "Epoch: 850 loss: 0.003\n",
      "[0.95024133 0.06198143 0.06198143 0.96407237]\n",
      "Epoch: 850 loss: 0.003\n",
      "[0.95027529 0.0619959  0.0619959  0.96409822]\n",
      "Epoch: 850 loss: 0.003\n",
      "[0.95064915 0.06164694 0.06164694 0.9643483 ]\n",
      "Epoch: 860 loss: 0.003\n",
      "[0.95061568 0.0616057  0.0616057  0.96432377]\n",
      "Epoch: 860 loss: 0.003\n",
      "[0.95058223 0.06156453 0.06156453 0.96429926]\n",
      "Epoch: 860 loss: 0.003\n",
      "[0.95061556 0.06157873 0.06157874 0.96432464]\n",
      "Epoch: 860 loss: 0.003\n",
      "[0.95098223 0.06123644 0.06123644 0.96457008]\n",
      "Epoch: 870 loss: 0.003\n",
      "[0.9509494  0.06119598 0.06119598 0.96454602]\n",
      "Epoch: 870 loss: 0.003\n",
      "[0.95091659 0.06115559 0.0611556  0.96452196]\n",
      "Epoch: 870 loss: 0.003\n",
      "[0.9509493  0.06116954 0.06116954 0.96454688]\n",
      "Epoch: 870 loss: 0.003\n",
      "[0.95130899 0.0608337  0.0608337  0.96478783]\n",
      "Epoch: 880 loss: 0.003\n",
      "[0.95127678 0.06079399 0.060794   0.96476421]\n",
      "Epoch: 880 loss: 0.003\n",
      "[0.95124459 0.06075436 0.06075437 0.9647406 ]\n",
      "Epoch: 880 loss: 0.003\n",
      "[0.95127672 0.06076805 0.06076806 0.96476507]\n",
      "Epoch: 880 loss: 0.003\n",
      "[0.95162964 0.06043847 0.06043848 0.96500165]\n",
      "Epoch: 890 loss: 0.003\n",
      "[0.95159803 0.06039951 0.06039951 0.96497846]\n",
      "Epoch: 890 loss: 0.003\n",
      "[0.95156645 0.06036061 0.06036062 0.96495529]\n",
      "Epoch: 890 loss: 0.003\n",
      "[0.95159799 0.06037406 0.06037406 0.96497932]\n",
      "Epoch: 890 loss: 0.003\n",
      "[0.95194435 0.06005054 0.06005055 0.96521167]\n",
      "Epoch: 900 loss: 0.003\n",
      "[0.95191333 0.06001229 0.0600123  0.9651889 ]\n",
      "Epoch: 900 loss: 0.003\n",
      "[0.95188233 0.05997411 0.05997411 0.96516615]\n",
      "Epoch: 900 loss: 0.003\n",
      "[0.95191332 0.05998732 0.05998732 0.96518976]\n",
      "Epoch: 900 loss: 0.003\n",
      "[0.95225332 0.05966969 0.0596697  0.965418  ]\n",
      "Epoch: 910 loss: 0.003\n",
      "[0.95222286 0.05963213 0.05963214 0.96539564]\n",
      "Epoch: 910 loss: 0.003\n",
      "[0.95219243 0.05959464 0.05959464 0.96537329]\n",
      "Epoch: 910 loss: 0.003\n",
      "[0.95222287 0.05960762 0.05960762 0.9653965 ]\n",
      "Epoch: 910 loss: 0.003\n",
      "[0.95255669 0.05929571 0.05929571 0.96562074]\n",
      "Epoch: 920 loss: 0.003\n",
      "[0.95252679 0.05925882 0.05925883 0.96559878]\n",
      "Epoch: 920 loss: 0.003\n",
      "[0.95249691 0.059222   0.059222   0.96557682]\n",
      "Epoch: 920 loss: 0.003\n",
      "[0.95252682 0.05923475 0.05923475 0.96559963]\n",
      "Epoch: 920 loss: 0.003\n",
      "[0.95285465 0.05892839 0.0589284  0.96582   ]\n",
      "Epoch: 930 loss: 0.003\n",
      "[0.95282528 0.05889216 0.05889216 0.96579842]\n",
      "Epoch: 930 loss: 0.003\n",
      "[0.95279593 0.05885599 0.05885599 0.96577685]\n",
      "Epoch: 930 loss: 0.003\n",
      "[0.95282533 0.05886852 0.05886852 0.96579927]\n",
      "Epoch: 930 loss: 0.003\n",
      "[0.95314734 0.05856755 0.05856756 0.96601587]\n",
      "Epoch: 940 loss: 0.003\n",
      "[0.95311849 0.05853195 0.05853196 0.96599466]\n",
      "Epoch: 940 loss: 0.003\n",
      "[0.95308966 0.05849641 0.05849642 0.96597346]\n",
      "Epoch: 940 loss: 0.003\n",
      "[0.95311856 0.05850873 0.05850874 0.96599551]\n",
      "Epoch: 940 loss: 0.003\n",
      "[0.95343492 0.058213   0.058213   0.96620844]\n",
      "Epoch: 950 loss: 0.003\n",
      "[0.95340657 0.05817802 0.05817802 0.9661876 ]\n",
      "Epoch: 950 loss: 0.003\n",
      "[0.95337824 0.05814309 0.0581431  0.96616676]\n",
      "Epoch: 950 loss: 0.003\n",
      "[0.95340666 0.05815521 0.05815521 0.96618845]\n",
      "Epoch: 950 loss: 0.003\n",
      "[0.95371752 0.05786456 0.05786457 0.96639782]\n",
      "Epoch: 960 loss: 0.002\n",
      "[0.95368967 0.05783018 0.05783018 0.96637733]\n",
      "Epoch: 960 loss: 0.002\n",
      "[0.95366182 0.05779585 0.05779585 0.96635684]\n",
      "Epoch: 960 loss: 0.002\n",
      "[0.95368977 0.05780777 0.05780777 0.96637817]\n",
      "Epoch: 960 loss: 0.002\n",
      "[0.95399529 0.05752207 0.05752207 0.96658408]\n",
      "Epoch: 970 loss: 0.002\n",
      "[0.95396791 0.05748827 0.05748827 0.96656393]\n",
      "Epoch: 970 loss: 0.002\n",
      "[0.95394055 0.05745452 0.05745452 0.96654379]\n",
      "Epoch: 970 loss: 0.002\n",
      "[0.95396804 0.05746624 0.05746624 0.96656477]\n",
      "Epoch: 970 loss: 0.002\n",
      "[0.95426837 0.05718535 0.05718535 0.9667673 ]\n",
      "Epoch: 980 loss: 0.002\n",
      "[0.95424145 0.05715211 0.05715212 0.96674749]\n",
      "Epoch: 980 loss: 0.002\n",
      "[0.95421455 0.05711893 0.05711893 0.96672768]\n",
      "Epoch: 980 loss: 0.002\n",
      "[0.95424159 0.05713046 0.05713046 0.96674833]\n",
      "Epoch: 980 loss: 0.002\n",
      "[0.95453687 0.05685425 0.05685425 0.96694758]\n",
      "Epoch: 990 loss: 0.002\n",
      "[0.95451041 0.05682157 0.05682157 0.96692809]\n",
      "Epoch: 990 loss: 0.002\n",
      "[0.95448395 0.05678893 0.05678894 0.96690861]\n",
      "Epoch: 990 loss: 0.002\n",
      "[0.95451056 0.05680028 0.05680028 0.96692893]\n",
      "Epoch: 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [-2,-1],\n",
    "    [25,6],\n",
    "    [17,4],\n",
    "    [-15,-6],\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "])\n",
    "network = NeuralNetwork()\n",
    "network.train(data,all_y_trues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is a boy\n"
     ]
    }
   ],
   "source": [
    "x = np.array([66,1000])\n",
    "x = network.feedforward(x)\n",
    "\n",
    "if x > 0.5:\n",
    "    print(\"she is a girl\")\n",
    "else:\n",
    "    print(\"he is a boy\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b4d75ac280b6c7c3aa43866cb82dc88915409b55fec83a093dd0284cb58708e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
